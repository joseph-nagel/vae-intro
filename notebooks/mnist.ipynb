{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "613d12d6-e74d-4573-81d8-5a108db5cbc5",
   "metadata": {},
   "source": [
    "# Bernoulli VAEs for MNIST\n",
    "\n",
    "After a short theory part in the introductory companion notebook, it is now time for a real demonstration. We therefore train two different VAEs with a (continuous) Bernoulli likelihood on MNIST images. The trained models are then imported in this notebook and analyzed with regards to their latent space representation as well as their compression, reconstruction and generation capabilities.\n",
    "\n",
    "A fully connected and a convolutional VAE can be trained by running the training script in the following ways:\n",
    "```\n",
    "python scripts/main.py fit --config config/mnist_dense.yaml\n",
    "\n",
    "python scripts/main.py fit --config config/mnist_conv.yaml\n",
    "```\n",
    "The training can be monitored by `tensorboard --logdir run/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e244cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e781632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from lightning.pytorch import seed_everything\n",
    "\n",
    "from varautoenc import(\n",
    "    MNISTDataModule,\n",
    "    ConvVAE,\n",
    "    DenseVAE,\n",
    "    generate,\n",
    "    reconstruct,\n",
    "    encode_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de042609-7f8a-4acc-823d-40aacae1e658",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = seed_everything(111111) # set random seeds manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39048686",
   "metadata": {},
   "source": [
    "## MNIST data\n",
    "\n",
    "Note that, as we have previously discussed, the **standard Bernoulli distribution** to strictly speaking only applicable to $\\{0,1\\}$-valued data. Since we use the **continuous Bernoulli distribution**, which is a technically valid choice for $[0,1]$-valued MNIST data, we do not need binarize the images beforehand as part of the preprocessing.\n",
    "\n",
    "The class `MNISTDataModule` manages the datasets and loaders corresponding to our setup. The latter ones allow for generating mini-batches of data during training and inference. Even though some random rotations are used for data augmentation purposes, it is remarked that one might of course apply more sophisticated random digit-preserving image transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc9c9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "binarized_mnist = MNISTDataModule(\n",
    "    data_dir='../run/data/',\n",
    "    binarize_threshold=None,\n",
    "    mean=None,\n",
    "    std=None,\n",
    "    batch_size=32,\n",
    ")\n",
    "\n",
    "binarized_mnist.prepare_data() # download data if not yet done\n",
    "binarized_mnist.setup(stage='test') # create test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff961c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = binarized_mnist.test_dataloader()\n",
    "x_batch, y_batch = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2ef4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(5, 4.5))\n",
    "for idx, ax in enumerate(axes.ravel()):\n",
    "    image = x_batch[idx, 0].numpy()\n",
    "    ax.imshow(image, cmap='gray', vmin=0, vmax=1)\n",
    "    ax.set_title(binarized_mnist.test_set.classes[y_batch[idx]])\n",
    "    ax.set(xticks=[], yticks=[], xlabel='', ylabel='')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05af819",
   "metadata": {},
   "source": [
    "## Dense VAE (two-dim. latent space)\n",
    "\n",
    "In a first experiment, we visualize the latent space of a densely connected VAE that is implemented in the class `DenseVAE`. The dimensionality of the latent space it set to only two. This is an arbitrary choice that facilitates the visual investigation of the encoding.\n",
    "\n",
    "As per our setup, both encoder and decoder have four dense layers each. Leaky ReLU activation functions are used where appropriate. The images are flattened when being passed to the encoder and reshaped to the original resolution by the decoder.\n",
    "\n",
    "A single Monte Carlo sample is used in order to estimate the objective for each data point. It was observed here that higher sample numbers have not improved convergence. The loss is noisy due to this and, as usual, the randomized mini-batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7076d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_file = '../run/mnist_dense/version_0/checkpoints/last.ckpt'\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "dense_vae = DenseVAE.load_from_checkpoint(ckpt_file)\n",
    "\n",
    "dense_vae = dense_vae.eval()\n",
    "dense_vae = dense_vae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5231bad-41ff-47ec-8dd0-62eca61feb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Likelihood: {dense_vae.likelihood_type}')\n",
    "print(f'Beta: {dense_vae.beta}')\n",
    "\n",
    "try:\n",
    "    sigma = dense_vae.decoder.dist_params.logsigma.exp().item()\n",
    "    print(f'Sigma: {sigma:.4f}')\n",
    "except AttributeError as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2d7a6b",
   "metadata": {},
   "source": [
    "After importing a trained model, the encoder is, facilitated by the function `encode_loader`, applied to the whole test set. Since this actually yields diagonal Gaussian distributions, only the mean vectors are used as the encodings. The alternative would be to take the full distributions or to simply generate random samples from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0a3d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mu, z_sigma, y = encode_loader(\n",
    "    dense_vae,\n",
    "    test_loader,\n",
    "    return_inputs=True\n",
    ")\n",
    "\n",
    "print('Latent mean: {:.2f}'.format(z_mu.mean()))\n",
    "print('Latent std.: {:.2f}'.format(z_mu.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64af7c1e",
   "metadata": {},
   "source": [
    "We can now plot the computed points in the two-dimensional latent space. Points belonging to different digits have different colors. While the VAE has not processed any class label information during training, it is interesting to observe how the classes are organized in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cc3a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5.5, 4))\n",
    "for idx in range(10):\n",
    "    ax.scatter(\n",
    "        z_mu[y==idx, 0][::2].numpy(),\n",
    "        z_mu[y==idx, 1][::2].numpy(),\n",
    "        color=plt.cm.tab10(idx), alpha=0.3,\n",
    "        edgecolors='none', label='y={}'.format(idx)\n",
    "    )\n",
    "ax.set(xlabel='$z_1$', ylabel='$z_2$', xlim=(-5, 7), ylim=(-5, 5))\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.legend(loc='center right')\n",
    "ax.grid(visible=True, which='both', color='lightgray', linestyle='-')\n",
    "ax.set_axisbelow(True)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf7f214",
   "metadata": {},
   "source": [
    "## Conv. VAE (32-dim. latent space)\n",
    "\n",
    "We investigate in a second experiment how to encode, reconstruct and generate data with the model class `ConvVAE` that utilizes both conv. and dense layers. While it takes significantly longer to train, the CNN-based architecture embodies certain prior assumptions (hierarchical representations, translation invariance) that are often reasonable in vision applications.\n",
    "\n",
    "The encoder passes the input through two conv. layers before the features are flattened and then passed through two dense layers. There are 32 latent variables. As far as possible, the decoder is constructed symmetrically. While the encoder contains standard downscaling operations, the decoder utilizes interpolation-based upscalings. Transposed convolutions would be an alternative, despite occasionally suffering from checkerboard artifacts. Again, a single-sample estimator of the loss and its gradient is employed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b8a56a-0680-4aa4-8888-c40b1de99227",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_file = '../run/mnist_conv/version_0/checkpoints/last.ckpt'\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "checkpoint = torch.load(ckpt_file, map_location=device)\n",
    "conv_vae = ConvVAE.load_from_checkpoint(ckpt_file)\n",
    "\n",
    "conv_vae = conv_vae.eval()\n",
    "conv_vae = conv_vae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd69958-f685-41d0-9a9b-8e989c9fd5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Likelihood: {conv_vae.likelihood_type}')\n",
    "print(f'Beta: {conv_vae.beta}')\n",
    "\n",
    "try:\n",
    "    sigma = conv_vae.decoder.dist_params.logsigma.exp().item()\n",
    "    print(f'Sigma: {sigma:.4f}')\n",
    "except AttributeError as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d436731-e752-402b-b72d-d7992335d9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mu, z_sigma = encode_loader(\n",
    "    conv_vae,\n",
    "    test_loader\n",
    ")\n",
    "\n",
    "print('Latent mean: {:.2f}'.format(z_mu.mean()))\n",
    "print('Latent std.: {:.2f}'.format(z_mu.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ed3635-f4fe-4322-b498-3ec08f288adb",
   "metadata": {},
   "source": [
    "### Image reconstruction\n",
    "\n",
    "Having imported a learned model, images can be compressed into their latent representation and subsequently be reconstructed. The function `reconstruct` computes the encoding while sampling is turned off. This means that the mean values of the Gaussian posterior form the encoding. The decoder is used on this latent code. Finally, the predicted Bernoulli probabilities constitute the image reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4987daff-ddc2-4328-8c6b-94e8853ffd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, _ = next(iter(test_loader))\n",
    "x_recon = reconstruct(conv_vae, x_batch, sample_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa53de3f",
   "metadata": {},
   "source": [
    "A random sample of the binarized test images is shown in the first row of the following figure. The second row contains the corresponding reconstructions. While the reconstruction quality seems to be generally satisfactory, one can observe a certain \"blurriness\". This is not only a consequence of comparing continuous reconstructions with binary originals, but also a general weakness of VAE models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100ae7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=6, figsize=(8, 3.5))\n",
    "\n",
    "for idx, ax in enumerate(axes[0]):\n",
    "    image = x_batch[idx, 0].numpy()\n",
    "    ax.imshow(image, cmap='gray', vmin=0, vmax=1)\n",
    "    ax.set_title('$x^{{({})}}$'.format(idx + 1))\n",
    "    ax.set(xticks=[], yticks=[], xlabel='', ylabel='')\n",
    "\n",
    "for idx, ax in enumerate(axes[1]):\n",
    "    image = x_recon[idx, 0].numpy().clip(0, 1)\n",
    "    ax.imshow(image, cmap='gray', vmin=0, vmax=1)\n",
    "    ax.set_title('$\\\\hat{{x}}^{{({})}}$'.format(idx + 1))\n",
    "    ax.set(xticks=[], yticks=[], xlabel='', ylabel='')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704bf145",
   "metadata": {},
   "source": [
    "### Random generation\n",
    "\n",
    "The function `generate` randomly generates new images. One hundred samples are drawn from the prior sampling distribution of the latent variables. This straightforward since the prior is just a diagonal Gaussian. Those latent values are subsequently passed to the decoder that predicts the Bernoulli probabilities. They are taken as the generated images without any further sampling or thresholding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbf1bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 25\n",
    "\n",
    "num_latents = conv_vae.decoder.dense_layers[0][0].in_features\n",
    "x_gen = generate(conv_vae, sample_shape=(num_latents,), num_samples=num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de71c74e",
   "metadata": {},
   "source": [
    "Even though the training data was binary-valued, the images generated that are generated this way have continuous pixel intensities. This makes them look smoother than the training data. While the results are certainly not perfect yet, the generated images look already quite realistic. Architectural improvements would likely enhance the generative modeling quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4961917",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=5, ncols=5, figsize=(5, 5))\n",
    "for idx, ax in enumerate(axes.ravel()):\n",
    "    image = x_gen[idx, 0].numpy().clip(0, 1)\n",
    "    ax.imshow(image, cmap='gray', vmin=0, vmax=1)\n",
    "    ax.set(xticks=[], yticks=[], xlabel='', ylabel='')\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
